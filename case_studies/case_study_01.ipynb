{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pathlib\n",
    "import pandas as pd\n",
    "import re\n",
    "import polars as pl\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "rc('font',**{'family':'sans-serif','sans-serif':['Helvetica']})\n",
    "plt.rcParams['svg.fonttype'] = 'none'\n",
    "\n",
    "import sys\n",
    "from alada import chap01 as ch01\n",
    "from alada import casestudy01 as cs01\n",
    "\n",
    "# Create data folder for the case study\n",
    "cs01.create_data_folder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case Study 01: Clustering of Doctors' Notes\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first case study we will be looking at as part of the ALADA couse is the problem of clustering medical notes using measures of similarities between vectors. We make use of a kaggle dataset for this purpose. The first step to running this notebook is to download the dataset and copy it into the data directory `data/case_study_01/`. \n",
    "\n",
    "Link to the kaggle dataset: [https://www.kaggle.com/datasets/gauravmodi/doctors-notes/data](https://www.kaggle.com/datasets/gauravmodi/doctors-notes/data)\n",
    "\n",
    "Download the reports.csv file, and copy it into the folder `data/case_study_01/`. After you have done this, run the following cell and check if it reports success."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Success! You can run this notebook.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cs01.check_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. What does this dataset have?\n",
    "\n",
    "The data file is a `.csv` file. If you do not know that means, then take a look [here](https://en.wikipedia.org/wiki/Comma-separated_values).\n",
    "\n",
    "The file has two columns: `medical_specialty` and `report`.\n",
    "- The `medical_specialty` column contains the specialty of the doctor who wrote the report, and\n",
    "- the `report` column contains the actual text of the report written down by the doctor for a particular patient.\n",
    "\n",
    "For the purpose of this case study, we will only be using the `report` column; we will use the `medical_specialty` column only for evaluating the clustering results for us know how well out clustering algorithm has done.\n",
    "\n",
    "You are probably asking at this point, we have learned about clustering using n-vectors; objects containing numbers. How does one cluster text data? We do that by first converting text data into vectors that represent, at least partially, the information contaning in the text. There are many ways to do this, but will make use a simple technique called `count vectorization`.\n",
    "\n",
    "**Count Vectorization** is a technique that converts text data into vectors by counting the number of times a set of `tokens` (chosen words) appears in the text. Tokens are words that are chosen from the text or given by a specialist from the field of study. Consider the following example:\n",
    "\n",
    "**Text to convert:** \"The quick brown fox jumps over the lazy dog. The dog is very lazy, but the fox is quick. The dog is friendly through.\"\n",
    "\n",
    "**Tokens:** [\"quick\", \"brown\", \"fox\", \"jumps\", \"lazy\", \"dog\"]\n",
    "\n",
    "We simply count the number of times each token appears in the text. The vector representation of the text is then: [2, 1, 2, 1, 2, 3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Clustering medical notes\n",
    "\n",
    "This problem deals with clsutering medical notes based on the test content. The \n",
    "original dataset is from Kaggle repository: https://www.kaggle.com/c/medical-notes\n",
    "\n",
    "This data is in the file `data/reorts.csv`.\n",
    "\n",
    "The data comes in a simple CSV format with two columns: `medical_specialty` and `report`.\n",
    "\n",
    "I have added a header to this file by generating a unique set of tokens identified from the original data set.\n",
    "\n",
    "The intial subsection has the unique token generation code, and the following subsection performs the clustering by ignoring the actual `medical_specialty` column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Generate unique tokens\n",
    "\n",
    "This is quite simple. We collect reports from each `medical_speciality` and makesa list of all unique words. Following this, only the words that unique occur in each speciality are retained, and then rest are dropped."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Supporting Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ErrorException",
     "evalue": "syntax: extra token \"count_tokens\" after end of expression",
     "output_type": "error",
     "traceback": [
      "syntax: extra token \"count_tokens\" after end of expression\n",
      "\n",
      "Stacktrace:\n",
      " [1] top-level scope\n",
      "   @ ~/Documents/work/teaching/course_material/ALADA-Course/case_studies/jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_X10sZmlsZQ==.jl:1"
     ]
    }
   ],
   "source": [
    "def count_tokens(note: str, tokens: list) -> np.array:\n",
    "    \"\"\"Find the token count in the medical records.\"\"\"\n",
    "    return np.array([note.count(tok) for tok in tokens])\n",
    "ss\n",
    "def get_all_text_for_medspec(medrec: pl.DataFrame, col: str) -> dict:\n",
    "    \"\"\"Combine all text for each medical specialty.\"\"\"\n",
    "    return {k: ' '.join(v[col].values) for k, v in medrec.groupby(\"medical_specialty\")}\n",
    "\n",
    "def get_tokens_for_medspec(medrec: pl.DataFrame, col: str, minlen: int=5) -> dict:\n",
    "    \"\"\"Get the tokens of length greater than minlen for each medical specialty.\"\"\"\n",
    "    pattern = re.compile(r'[^\\w\\s]|\\d')\n",
    "    alltext = get_all_text_for_medspec(medrec, col)\n",
    "    return {\n",
    "        ms: [s for s in np.unique(txt.split()) if not pattern.search(s) and len(s) > minlen]\n",
    "        for ms, txt in alltext.items()\n",
    "    }\n",
    "\n",
    "def get_unique_tokens_for_medspec(ms_tokens: dict) -> dict:\n",
    "    \"\"\"Get unique tokens for each medical specialty.\"\"\"\n",
    "    ms_unique_tokens = {}\n",
    "    for _k, _v in ms_tokens.items():\n",
    "        # Find the unique tokens for each medical specialty\n",
    "        _toks = set(_v)\n",
    "        for _ms in ms_tokens.keys():\n",
    "            if _ms != _k:\n",
    "                _toks -= set(ms_tokens[_ms])\n",
    "        ms_unique_tokens[_k] = list(_toks)\n",
    "    return ms_unique_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code to generate unique tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "UndefVarError",
     "evalue": "UndefVarError: pathlib not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: pathlib not defined\n",
      "\n",
      "Stacktrace:\n",
      " [1] top-level scope\n",
      "   @ ~/Documents/work/teaching/course_material/ALADA-Course/case_studies/jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_X12sZmlsZQ==.jl:2"
     ]
    }
   ],
   "source": [
    "# Read header for tokens\n",
    "datafile = (datadir / pathlib.Path(\"reports.csv\")).as_posix()\n",
    "with open(datafile, \"r\") as f:\n",
    "    tokens = [_s.strip() for _s in f.readline().split(':')[-1].split(\",\")]\n",
    "\n",
    "# Read the medical records file.\n",
    "medrec = pd.read_csv(datafile, skiprows=2)\n",
    "medrec[\"medical_specialty\"].value_counts()\n",
    "# Make the reports column lower case\n",
    "medrec[\"report_lower\"] = medrec[\"report\"].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "UndefVarError",
     "evalue": "UndefVarError: get_tokens_for_medspec not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: get_tokens_for_medspec not defined\n",
      "\n",
      "Stacktrace:\n",
      " [1] top-level scope\n",
      "   @ ~/Documents/work/teaching/course_material/ALADA-Course/case_studies/jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_X13sZmlsZQ==.jl:2"
     ]
    }
   ],
   "source": [
    "# Get all potential tokens for each medical specialty\n",
    "ms_tokens = get_tokens_for_medspec(medrec, col=\"report_lower\")\n",
    "ms_unique_tokens = get_unique_tokens_for_medspec(ms_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "UndefVarError",
     "evalue": "UndefVarError: get_all_text_for_medspec not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: get_all_text_for_medspec not defined\n",
      "\n",
      "Stacktrace:\n",
      " [1] top-level scope\n",
      "   @ ~/Documents/work/teaching/course_material/ALADA-Course/case_studies/jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_X14sZmlsZQ==.jl:2"
     ]
    }
   ],
   "source": [
    "# All text for each medical specialty\n",
    "ms_alltext = get_all_text_for_medspec(medrec, col=\"report_lower\")\n",
    "ms_token_count = {}\n",
    "for k, txt in ms_alltext.items():\n",
    "    _cnts = [txt.count(_t) for _t in ms_unique_tokens[k]]\n",
    "    # Polar dataframe from the counts dictionary\n",
    "    ms_token_count[k] = pl.DataFrame(\n",
    "        data={\"token\": ms_unique_tokens[k], \"count\": _cnts},\n",
    "        schema={\"token\": pl.String, \"count\": pl.Int64}\n",
    "    ).sort(\"count\", descending=True)\n",
    "\n",
    "# Choose the top 15 tokens for each medical specialty\n",
    "top_tokens = []\n",
    "for k, v in ms_token_count.items():\n",
    "    top_tokens += v.head(5)[\"token\"].to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Clustering medical rpoets through `kmeans`\n",
    "\n",
    "We now assume that we only have access to the medical reports and not to the speciality they correspond to. We make use of the unique tokens to count the frequency of each of these tokens in each report to generate a set of column vectors for each report. This column vector is then used to compare the similarity between reports, and cluster them.\n",
    "\n",
    "There are many entries from the surgery medical speciality, and thus this will be removed from the data for the clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "UndefVarError",
     "evalue": "UndefVarError: pathlib not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: pathlib not defined\n",
      "\n",
      "Stacktrace:\n",
      " [1] top-level scope\n",
      "   @ ~/Documents/work/teaching/course_material/ALADA-Course/case_studies/jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_X16sZmlsZQ==.jl:1"
     ]
    }
   ],
   "source": [
    "datadir = pathlib.Path(\"data\")\n",
    "outdir = pathlib.Path(\"output\")\n",
    "outdir.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "UndefVarError",
     "evalue": "UndefVarError: pathlib not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: pathlib not defined\n",
      "\n",
      "Stacktrace:\n",
      " [1] top-level scope\n",
      "   @ ~/Documents/work/teaching/course_material/ALADA-Course/case_studies/jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_X20sZmlsZQ==.jl:2"
     ]
    }
   ],
   "source": [
    "# Read header for tokens\n",
    "datafile = (datadir / pathlib.Path(\"reports.csv\")).as_posix()\n",
    "with open(datafile, \"r\") as f:\n",
    "    tokens = [_s.strip() for _s in f.readline().split(':')[-1].split(\",\")]\n",
    "\n",
    "# Read the medical records file.\n",
    "medrec = pd.read_csv(datafile, skiprows=2)\n",
    "medrec[\"medical_specialty\"].value_counts()\n",
    "# Make the reports column lower case\n",
    "medrec[\"report_lower\"] = medrec[\"report\"].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "UndefVarError",
     "evalue": "UndefVarError: medrec not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: medrec not defined\n",
      "\n",
      "Stacktrace:\n",
      " [1] top-level scope\n",
      "   @ ~/Documents/work/teaching/course_material/ALADA-Course/case_studies/jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_X21sZmlsZQ==.jl:2"
     ]
    }
   ],
   "source": [
    "# Get the token vectors for each medical note.\n",
    "inx = medrec[\"medical_specialty\"] != \"Surgery\"\n",
    "medrec = medrec[inx]\n",
    "medrec.reset_index(inplace=True, drop=True)\n",
    "tokvec = np.array([count_tokens(_v, tokens) for _v in medrec['report_lower'].values])\n",
    "tokvec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 4\n",
    "km = chap01.KMeans(X=tokvec, k=4)\n",
    "cm, ca, j = km.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the medical specialty for each cluster\n",
    "medrec[\"cluster\"] = ca[-1]\n",
    "# Print value counts for different clusters.\n",
    "for _c in range(k):\n",
    "    print(f\"Cluster {_c}: {np.linalg.norm(cm[-1][_c]): .3f}\")\n",
    "    print(medrec[medrec[\"cluster\"] == _c][\"medical_specialty\"].value_counts())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(cm[-1].T + np.array([0, 1, 2, 3]));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering using k-means\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cluster_assignment(tvec, mvec):\n",
    "    return np.argmin(np.linalg.norm(mvec - tvec, axis=1))\n",
    "\n",
    "def get_cluster_mean(tokvec, clustassign):\n",
    "    return np.array([np.mean(tokvec[clustassign == _c, :], axis=0)\n",
    "                     for _c in np.unique(clustassign)])\n",
    "\n",
    "def get_j_clust(tokvec, mvec, clustassign):\n",
    "    return np.sum([np.sum(np.square(np.linalg.norm(tokvec[clustassign == _c, :] - mvec[_i, :], axis=1))) for _i, _c in enumerate(np.unique(clustassign))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 3\n",
    "# Random starting means.\n",
    "mvec = tokvec[list(np.random.randint(0, tokvec.shape[0], k)), :]\n",
    "\n",
    "# Cluster assingment for the random means.\n",
    "clustassign = np.array([get_cluster_assignment(_tvec, mvec) for _tvec in tokvec])\n",
    "\n",
    "# Compute J_clust\n",
    "J_clust_curr = get_j_clust(tokvec, mvec, clustassign)\n",
    "\n",
    "# Iterate now.\n",
    "n_iter = 20\n",
    "for i in range(n_iter):\n",
    "    # Update the means.\n",
    "    mvec = get_cluster_mean(tokvec, clustassign)\n",
    "    \n",
    "    # Update the cluster assignment.\n",
    "    clustassign = np.array([get_cluster_assignment(_tvec, mvec) for _tvec in tokvec])\n",
    "    \n",
    "    # Update J_clust\n",
    "    J_clust_prev = J_clust_curr\n",
    "    J_clust_curr = get_j_clust(tokvec, mvec, clustassign)\n",
    "\n",
    "    print(f\"Iteration: {i:2d}, J_clust: {J_clust_curr:6.2f}, Change: {100 * (J_clust_curr - J_clust_prev) / J_clust_prev:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for performing k-means algorithm \n",
    "def get_cluster_assignment(tvec, mvec):\n",
    "    return np.argmin(np.linalg.norm(mvec - tvec, axis=1))\n",
    "\n",
    "def get_cluster_mean(tokvec, clustassign):\n",
    "    return np.array([np.mean(tokvec[clustassign == _c, :], axis=0)\n",
    "                     for _c in np.unique(clustassign)])\n",
    "\n",
    "def get_j_clust(tokvec, mvec, clustassign):\n",
    "    return np.sum([np.sum(np.square(np.linalg.norm(tokvec[clustassign == _c, :] - mvec[_i, :], axis=1))) for _i, _c in enumerate(np.unique(clustassign))])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo of kmean class from chap01 module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sample data\n",
    "_x1 = np.random.randn(100, 2) + np.array([4, 4])\n",
    "_x2 = np.random.randn(100, 2)\n",
    "X = np.vstack([_x1, _x2])\n",
    "# Randomly reorder the rows\n",
    "np.random.shuffle(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=2\n",
    "km = chap01.KMeans(X, 2)\n",
    "cm, ca, j = km.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the k-mean algorithm evolution.\n",
    "colors = [\"tab:blue\", \"tab:red\", \"tab:green\", \"tab:orange\", \"tab:purple\", \"tab:brown\", \"tab:pink\", \"tab:gray\", \"tab:olive\", \"tab:cyan\"]\n",
    "n = len(cm)\n",
    "m = (n // 5) + 1\n",
    "fig = plt.figure(figsize=(15, 2.5 * m))\n",
    "for i in range(n):\n",
    "    ax = fig.add_subplot(m, 5, i + 1)\n",
    "    for _k in range(k):\n",
    "        ax.scatter(X[ca[i] == _k, 0], X[ca[i] == _k, 1], s=25, color=colors[_k],\n",
    "                   marker=\"x\", alpha=0.2)\n",
    "    for _k in range(k):\n",
    "        ax.scatter(cm[i][_k, 0], cm[i][_k, 1], s=50, marker=\"o\", alpha=1, \n",
    "                   color=colors[_k], edgecolors=\"black\") \n",
    "    ax.set_title(f\"Iter: {i + 1}: J = {j[i]:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Similarity problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([\n",
    "    [167, 102, 36.6],\n",
    "    [180, 87, 26.9],\n",
    "    [177, 78, 24.9],\n",
    "    [152, 76, 32.9],\n",
    "]).T\n",
    "\n",
    "# Compute the distance between the points.\n",
    "np.array([[np.linalg.norm(_v1 - _v2) for _v2 in X.T] for _v1 in X.T])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = X * np.array([0.01, 1, 1]).reshape(-1, 1)\n",
    "# Compute the distance between the points.\n",
    "np.array([[np.linalg.norm(_v1 - _v2) for _v2 in X1.T] for _v1 in X1.T])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Angle similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def angle_between(v1, v2):\n",
    "    return (180 / np.pi) * np.arccos(np.max([-1, np.min([1, np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))])]))\n",
    "\n",
    "# Compute the distance between the points.\n",
    "np.round(100 * np.array([[angle_between(_v1, _v2) for _v2 in X.T] for _v1 in X.T])) / 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the distance between the points.\n",
    "np.round(100 * np.array([[angle_between(_v1, _v2) for _v2 in X1.T] for _v1 in X1.T])) / 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.round(100 * np.array([v / np.linalg.norm(v) for v in X.T]).T) / 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.round(100 * np.array([v / np.linalg.norm(v) for v in X1.T]).T) / 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alada",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
